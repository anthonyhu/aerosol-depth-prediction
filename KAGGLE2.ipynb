{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# IMPORT DATA\n",
    "\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].replace(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "train, test = load_data()\n",
    "random_seed = 8888\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 22 RMSE: 0.664492983216352\n",
      "Pruning 23 RMSE: 0.6592802332252014\n",
      "Pruning 24 RMSE: 0.6488767212490174\n",
      "Pruning 25 RMSE: 0.6440220044258622\n",
      "Pruning 26 RMSE: 0.6448665249174638\n",
      "Pruning 26 RMSE: 0.6337929111353886\n",
      "Pruning 27 RMSE: 0.6338485033862755\n",
      "Validation fold 1 RMSE: 0.6338485033862755\n",
      "Pruning 27 RMSE: 0.6408909647727296\n",
      "Pruning 28 RMSE: 0.6341245295759226\n",
      "Pruning 29 RMSE: 0.6252991499928274\n",
      "Pruning 30 RMSE: 0.6358270130724287\n",
      "Pruning 31 RMSE: 0.6321307778980557\n",
      "Pruning 32 RMSE: 0.6178406352471559\n",
      "Pruning 33 RMSE: 0.6193021544132367\n",
      "Pruning 34 RMSE: 0.6228607269236701\n",
      "Pruning 35 RMSE: 0.6216035344986838\n",
      "Pruning 36 RMSE: 0.6139450289933974\n",
      "Pruning 37 RMSE: 0.612540027019624\n",
      "Pruning 38 RMSE: 0.6048404001593066\n",
      "Pruning 39 RMSE: 0.6113359998142704\n",
      "Pruning 40 RMSE: 0.6164687194331823\n",
      "Pruning 41 RMSE: 0.6149720517769527\n",
      "Pruning 42 RMSE: 0.6112682501319839\n",
      "Pruning 43 RMSE: 0.6132182251822867\n",
      "Pruning 44 RMSE: 0.6038391880532306\n",
      "Pruning 45 RMSE: 0.6140527032296857\n",
      "Pruning 46 RMSE: 0.620205506612193\n",
      "Pruning 47 RMSE: 0.615456988363486\n",
      "Pruning 48 RMSE: 0.6130454617169521\n",
      "Pruning 49 RMSE: 0.6303626379029205\n",
      "Pruning 50 RMSE: 0.6165097724986406\n",
      "Pruning 51 RMSE: 0.6142186081616926\n",
      "Pruning 52 RMSE: 0.6121454527895385\n",
      "Pruning 53 RMSE: 0.6131084076879445\n",
      "Pruning 53 RMSE: 0.7413268477411694\n",
      "Pruning 54 RMSE: 0.7682785480819658\n",
      "Pruning 55 RMSE: 0.7618181157541679\n",
      "Pruning 56 RMSE: 0.7559113180933502\n",
      "Pruning 57 RMSE: 0.7509490579806263\n",
      "Pruning 58 RMSE: 0.7475501263600572\n",
      "Pruning 59 RMSE: 0.7440687006723208\n",
      "Pruning 60 RMSE: 0.7405602357628124\n",
      "Pruning 61 RMSE: 0.7377106012846153\n",
      "Pruning 62 RMSE: 0.7353426026547256\n",
      "Pruning 63 RMSE: 0.7320732084703289\n",
      "Pruning 64 RMSE: 0.7319705242490793\n",
      "Validation fold 2 RMSE: 0.7319705242490793\n",
      "Pruning 64 RMSE: 0.6488505079757299\n",
      "Pruning 65 RMSE: 0.6434872445877939\n",
      "Pruning 66 RMSE: 0.6310458282901151\n",
      "Pruning 67 RMSE: 0.6292428629566471\n",
      "Pruning 68 RMSE: 0.633601489360879\n",
      "Pruning 69 RMSE: 0.6238194554805117\n",
      "Pruning 70 RMSE: 0.6225446182487343\n",
      "Pruning 71 RMSE: 0.6177262308411567\n",
      "Pruning 72 RMSE: 0.6135760949288577\n",
      "Pruning 73 RMSE: 0.6146768439304368\n",
      "Pruning 74 RMSE: 0.6154683386075777\n",
      "Pruning 74 RMSE: 0.6861868435329201\n",
      "Pruning 75 RMSE: 0.6808682553461809\n",
      "Pruning 76 RMSE: 0.6820326426672335\n",
      "Pruning 77 RMSE: 0.6838551085194511\n",
      "Pruning 78 RMSE: 0.6844541790848199\n",
      "Validation fold 3 RMSE: 0.6844541790848199\n",
      "Pruning 78 RMSE: 0.6526593017004092\n",
      "Pruning 79 RMSE: 0.6514733559376917\n",
      "Pruning 80 RMSE: 0.6437364992743794\n",
      "Pruning 81 RMSE: 0.6317982362206362\n",
      "Pruning 82 RMSE: 0.6397395488547247\n",
      "Pruning 83 RMSE: 0.6369937309255236\n",
      "Pruning 84 RMSE: 0.6403088670650711\n",
      "Pruning 85 RMSE: 0.6378036929297567\n",
      "Pruning 86 RMSE: 0.6392100454700151\n",
      "Pruning 87 RMSE: 0.6313880462741263\n",
      "Pruning 88 RMSE: 0.6466838107977646\n",
      "Pruning 89 RMSE: 0.6259710275944768\n",
      "Pruning 90 RMSE: 0.6323042694172993\n",
      "Pruning 91 RMSE: 0.6327805675294924\n",
      "Pruning 91 RMSE: 0.6511077606752865\n",
      "Pruning 92 RMSE: 0.6606604014530113\n",
      "Pruning 93 RMSE: 0.6576485898429715\n",
      "Pruning 94 RMSE: 0.6556420303516316\n",
      "Pruning 95 RMSE: 0.6540758527410031\n",
      "Pruning 96 RMSE: 0.6530916643446238\n",
      "Validation fold 4 RMSE: 0.6530916643446238\n",
      "Pruning 96 RMSE: 0.6351673962441012\n",
      "Pruning 97 RMSE: 0.6273163803399148\n",
      "Pruning 98 RMSE: 0.6262877520011942\n",
      "Pruning 99 RMSE: 0.6176195887689123\n",
      "Pruning 100 RMSE: 0.6114989611112661\n",
      "Pruning 101 RMSE: 0.611335691237747\n",
      "Pruning 101 RMSE: 0.7247040183662903\n",
      "Pruning 102 RMSE: 0.7222454766184021\n",
      "Pruning 103 RMSE: 0.7228166078974093\n",
      "Validation fold 5 RMSE: 0.7228166078974093\n",
      "Cross-validation score: 0.6852362957924416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "average_RMSE = 0.0\n",
    "n_splits = 5\n",
    "    \n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "nb_fold = 0\n",
    "for train_index, validation_index in kf.split(train):\n",
    "    nb_fold += 1\n",
    "    train_fold, validation_fold = train.loc[train_index], train.loc[validation_index]\n",
    "        \n",
    "\n",
    "    def input_fn(data_set):\n",
    "        feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "        labels = tf.constant(data_set[\"y\"].values)\n",
    "        return feature_cols, labels\n",
    "        \n",
    "      \n",
    "    model_dnn = MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                 max_iter=1000,\n",
    "                                 early_stopping=True,\n",
    "                                 alpha=parameters[\"l2_reg\"],\n",
    "                                 random_state=random_seed)\n",
    "    \n",
    "    model_dnn.fit(train_fold[cols_dnn], train_fold[\"y\"])\n",
    "\n",
    "    train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        \n",
    "    y_hat = model_dnn.predict(train_fold[cols_dnn])\n",
    "        \n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, RMSE))\n",
    "        \n",
    "    # Prune outliers\n",
    "    RMSE_decreasing = True\n",
    "    while (RMSE_decreasing):\n",
    "        count +=1\n",
    "        train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "        train_pred[\"y\"] = train_pred[\"id\"].map(train_fold[\"y\"])\n",
    "\n",
    "        # Distance from the median for each bag\n",
    "        train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "        #Rank of each instance by bag\n",
    "        train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "        bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "        train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "        train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "        # Remove outliers\n",
    "        outliers_index = train_pred[\"rank\"] > (1 - 0.02)\n",
    "        train_pred = train_pred.loc[~outliers_index, :].reset_index(drop=True)\n",
    "        train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "        \n",
    "        # Remove bags\n",
    "        y_hat_mean = train_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "        train_pred[\"y_mean\"] = train_pred[\"id\"].map(y_hat_mean)\n",
    "        train_pred[\"bag_score\"] = (train_pred[\"y_mean\"] - train_pred[\"y\"])**2\n",
    "        \n",
    "        train_pred[\"bag_rank\"] = train_pred.groupby(\"id\")[\"bag_score\"].rank()\n",
    "        number_of_bags = len(train_pred.id.unique())\n",
    "        train_pred[\"bag_rank\"] = train_pred[\"bag_rank\"] / number_of_bags\n",
    "        \n",
    "        outliers_index2 = train_pred[\"bag_rank\"] > (1 - 0.02)\n",
    "        #train_pred = train_pred.loc[~outliers_index2, :].reset_index(drop=True)\n",
    "        train_fold = train_fold.loc[~outliers_index2, :].reset_index(drop=True)\n",
    "            \n",
    "        model_dnn = MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                     max_iter=1000,\n",
    "                                     early_stopping=True,\n",
    "                                    alpha=parameters[\"l2_reg\"],\n",
    "                                     random_state=random_seed)\n",
    "        \n",
    "        model_dnn.fit(train_fold[cols_dnn], train_fold[\"y\"])\n",
    "            \n",
    "            # Compute new RMSE\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            \n",
    "        y_hat = model_dnn.predict(train_fold[cols_dnn])\n",
    "            #np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "        y_hat_mean = train_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "\n",
    "        new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_mean), train_fold[\"y\"]))\n",
    "        print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "        if (abs(new_RMSE - RMSE) > 0.0025):\n",
    "            \n",
    "            RMSE = new_RMSE\n",
    "        else:\n",
    "            RMSE_decreasing = False\n",
    "        \n",
    "    validation_pred = validation_fold[[\"id\"]].assign(y_hat=0)\n",
    "    \n",
    "    y_hat = model_dnn.predict(validation_fold[cols_dnn])\n",
    "            #np.array(list(itertools.islice(temp, 0, None)))\n",
    "        \n",
    "    # PRUNE VAL SET\n",
    "        \n",
    "    validation_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(validation_pred[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, RMSE))\n",
    "        \n",
    "    # Prune outliers\n",
    "    RMSE_decreasing = True\n",
    "    while (RMSE_decreasing):\n",
    "        count +=1\n",
    "        validation_pred[\"y_med\"] = validation_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "        # Distance from the median for each bag\n",
    "        validation_pred[\"score\"] = (validation_pred[\"y_hat\"] - validation_pred[\"y_med\"])**2\n",
    "        #Rank of each instance by bag\n",
    "        validation_pred[\"rank\"] = validation_pred.groupby(\"id\")[\"score\"].rank()\n",
    "        bag_size_dict = validation_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "        validation_pred[\"bag_size\"] = validation_pred[\"id\"].map(bag_size_dict)\n",
    "        validation_pred[\"rank\"] = validation_pred[\"rank\"] / validation_pred[\"bag_size\"]\n",
    "\n",
    "        # Remove outliers\n",
    "        outliers_index = validation_pred[\"rank\"] > (1 - 0.02)\n",
    "        #validation_pred = validation_pred.loc[~outliers_index, :].reset_index(drop=True)\n",
    "        validation_fold = validation_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "            \n",
    "            # Compute new RMSE\n",
    "        validation_pred = validation_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            \n",
    "        y_hat = model_dnn.predict(validation_fold[cols_dnn])\n",
    "            #np.array(list(itertools.islice(temp, 0, None)))\n",
    "        validation_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "        y_hat_mean = validation_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "\n",
    "        new_RMSE = np.sqrt(mean_squared_error(validation_pred[\"id\"].map(y_hat_mean), validation_fold[\"y\"]))\n",
    "        print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "        if (abs(new_RMSE - RMSE) > 0.0025):\n",
    "            \n",
    "            RMSE = new_RMSE\n",
    "        else:\n",
    "            RMSE_decreasing = False\n",
    "            \n",
    "    y_hat = model_dnn.predict(validation_fold[cols_dnn])\n",
    "    \n",
    "    validation_pred[\"y_hat\"] = y_hat\n",
    "    y_hat_mean = validation_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "    RMSE = np.sqrt(mean_squared_error(validation_fold[\"id\"].map(y_hat_mean).values, validation_fold[\"y\"]))\n",
    "        \n",
    "    average_RMSE += RMSE\n",
    "    \n",
    "    print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "average_RMSE /= n_splits\n",
    "\n",
    "print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL \n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "        \n",
    "      \n",
    "model_dnn = MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                 max_iter=1000,\n",
    "                                 early_stopping=True,\n",
    "                                 alpha=parameters[\"l2_reg\"],\n",
    "                                 random_state=random_seed)\n",
    "    \n",
    "model_dnn.fit(train_fold[cols_dnn], train_fold[\"y\"])\n",
    "\n",
    "train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        \n",
    "y_hat = model_dnn.predict(train_fold[cols_dnn])\n",
    "        \n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "print(\"Pruning {0} RMSE: {1}\".format(count, RMSE))\n",
    "        \n",
    "# Prune outliers\n",
    "RMSE_decreasing = True\n",
    "while (RMSE_decreasing):\n",
    "    count +=1\n",
    "    train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "    train_pred[\"y\"] = train_pred[\"id\"].map(train_fold[\"y\"])\n",
    "\n",
    "    # Distance from the median for each bag\n",
    "    train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "    #Rank of each instance by bag\n",
    "    train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "    bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "    train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "    train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    outliers_index = train_pred[\"rank\"] > (1 - 0.02)\n",
    "    train_pred = train_pred.loc[~outliers_index, :].reset_index(drop=True)\n",
    "    train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "        \n",
    "    # Remove bags\n",
    "    y_hat_mean = train_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "    train_pred[\"y_mean\"] = train_pred[\"id\"].map(y_hat_mean)\n",
    "    train_pred[\"bag_score\"] = (train_pred[\"y_mean\"] - train_pred[\"y\"])**2\n",
    "        \n",
    "    train_pred[\"bag_rank\"] = train_pred.groupby(\"id\")[\"bag_score\"].rank()\n",
    "    number_of_bags = len(train_pred.id.unique())\n",
    "    train_pred[\"bag_rank\"] = train_pred[\"bag_rank\"] / number_of_bags\n",
    "        \n",
    "    outliers_index2 = train_pred[\"bag_rank\"] > (1 - 0.02)\n",
    "    #train_pred = train_pred.loc[~outliers_index2, :].reset_index(drop=True)\n",
    "    train_fold = train_fold.loc[~outliers_index2, :].reset_index(drop=True)\n",
    "            \n",
    "    model_dnn = MLPRegressor(hidden_layer_sizes=(10,),\n",
    "                                     max_iter=1000,\n",
    "                                     early_stopping=True,\n",
    "                                    alpha=parameters[\"l2_reg\"],\n",
    "                                     random_state=random_seed)\n",
    "        \n",
    "    model_dnn.fit(train_fold[cols_dnn], train_fold[\"y\"])\n",
    "            \n",
    "            # Compute new RMSE\n",
    "    train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            \n",
    "    y_hat = model_dnn.predict(train_fold[cols_dnn])\n",
    "            #np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "    y_hat_mean = train_pred.groupby(\"id\").mean()[\"y_hat\"].to_dict()\n",
    "\n",
    "    new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_mean), train_fold[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "    if (abs(new_RMSE - RMSE) > 0.0025):\n",
    "            \n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        RMSE_decreasing = False\n",
    "        \n",
    "validation_pred = validation_fold[[\"id\"]].assign(y_hat=0)\n",
    "    \n",
    "y_hat = model_dnn.predict(validation_fold[cols_dnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'C:\\\\Users\\\\pinouche\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\learn\\\\python\\\\learn\\\\preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2188331efbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# SET UP MODEL FIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_valued_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_dnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_dnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNNRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2188331efbb5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# SET UP MODEL FIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_valued_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_dnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_dnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNNRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[1;31m# Replace the lazy loader with the imported module itself.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcontrib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorflow.contrib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_editor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\factorization\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclustering_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorization_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgmm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgmm_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\gmm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmonitor_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mestimator_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_hooks\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mpath_hook_for_FileFinder\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isdir\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pinouche\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SET UP MODEL FIT\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, hidden_units=[10])\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'input_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d9722e024852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# MODEL FIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_dnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'input_fn'"
     ]
    }
   ],
   "source": [
    "# MODEL FIT\n",
    "\n",
    "model_dnn.fit(input_fn=lambda: input_fn(train), steps=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n"
     ]
    }
   ],
   "source": [
    "# GET THE COMPUTED TRAINED PREDICTIONS\n",
    "\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COMPUTE MEAN AND SD FOR EACH 100 Y_hat\n",
    "\n",
    "mean_y_hat = []\n",
    "sd_y_hat = []\n",
    "\n",
    "\n",
    "for i in range(1,981):\n",
    "    xx1 = (train_pred[train_pred[\"id\"] == i])\n",
    "    mean_y_hat.append((np.median(xx1.loc[:,\"y_hat\"])))\n",
    "    \n",
    "for i in range(1,981):\n",
    "    xx2 = (train_pred[train_pred[\"id\"] == i])\n",
    "    sd_y_hat.append(np.std(xx2.loc[:,\"y_hat\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COMPUTE THE CI 80%\n",
    "\n",
    "CI = []\n",
    "CI_upper = []\n",
    "CI_lower = []\n",
    "\n",
    "for i in range(0,980):\n",
    "    CI_upper.append(mean_y_hat[i]+(2.576*(sd_y_hat[i]/np.sqrt(np.count_nonzero([train_pred[\"id\"] == (i+1)])))))\n",
    "    CI_lower.append(mean_y_hat[i]-(2.576*(sd_y_hat[i]/np.sqrt(np.count_nonzero([train_pred[\"id\"] == (i+1)])))))\n",
    "\n",
    "    \n",
    "CI = ([CI_lower,CI_upper])     \n",
    "CI_t = np.transpose(CI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24523,)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET THE INDEX OF THE INSTANCES THAT ARE IN THE 80 % CI\n",
    "kept_instances = []\n",
    "\n",
    "for i in range(0,98000):\n",
    "    if ((train_pred.iloc[i,1] > CI_lower[int(np.floor((i/100)))]) & (train_pred.iloc[i,1] < CI_upper[int(np.floor((i/100)))])):\n",
    "        kept_instances.append(i)\n",
    "\n",
    "np.shape(kept_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2 = []\n",
    "\n",
    "for i in range(0,98000):\n",
    "    id2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID2 = [\"id2\"]\n",
    "id2_Array = np.asarray(id2)\n",
    "df_id2 = pd.DataFrame(id2_Array, columns=ID2)\n",
    "df_id2_y_hat = pd.concat([train_pred,df_id2], axis=1)\n",
    "\n",
    "df_kept_y_hat = df_id2_y_hat[df_id2_y_hat['id2'].isin(np.asarray(kept_instances))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the y with the y_hat\n",
    "y_values = []\n",
    "count_array = []\n",
    "count979 = []\n",
    "\n",
    "for i in range(0,980):\n",
    "    y_values.append(train.iloc[i*100,13])\n",
    "    count_array.append((np.count_nonzero(df_kept_y_hat[df_kept_y_hat[\"id\"] == (i+1)])/3))\n",
    "    \n",
    "new_y = np.repeat(y_values, count_array, axis=0)\n",
    "\n",
    "for i in range (1,981):\n",
    "    count979.append(i)\n",
    "    \n",
    "id3 = np.repeat(count979, count_array, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CONCATENATE AS A DATA FRAME\n",
    "Y = [\"y\"]\n",
    "new_index = []\n",
    "\n",
    "new_y_Array = np.asarray(new_y)\n",
    "df_new_y = pd.DataFrame(new_y_Array, columns=Y)\n",
    "array1 = np.asarray(df_kept_y_hat.loc[:,\"id\"])\n",
    "array2 = np.asarray(df_kept_y_hat.loc[:,\"y_hat\"])\n",
    "array3 = np.asarray(df_kept_y_hat.loc[:,\"id2\"])\n",
    "\n",
    "for i in range(0,np.shape(df_kept_y_hat)[0]):\n",
    "    new_index.append(str(i))\n",
    "\n",
    "# different indexing as df_kept_y_hat\n",
    "df_new_index = pd.DataFrame({'id': array1 ,'y_hat': array2, 'id2': array3},index=new_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPREAD BETWEEN mean of y_hat and true y\n",
    "id3_array = np.asarray(id3)\n",
    "df_id3 = pd.DataFrame(id3_array, columns=[\"id\"])\n",
    "df_trueY_withID = pd.concat([df_id3,df_new_y], axis=1)\n",
    "\n",
    "RMSE = []\n",
    "Difference = []\n",
    "\n",
    "for i in range(1,981):\n",
    "    xxx = (df_trueY_withID[df_trueY_withID[\"id\"] == i])\n",
    "    xxx2 = (df_new_index[df_new_index[\"id\"] == i])\n",
    "    mean_tmp1 = np.mean(xxx2.loc[:,\"y_hat\"])\n",
    "    mean_tmp2 = (xxx.loc[:,\"y\"])\n",
    "    Difference.append(mean_tmp1-mean_tmp2)\n",
    "    RMSE.append(np.sqrt(np.mean(np.sum(np.square(mean_tmp1-mean_tmp2)))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.681429</td>\n",
       "      <td>-0.668911</td>\n",
       "      <td>-0.263843</td>\n",
       "      <td>-0.168792</td>\n",
       "      <td>-0.060708</td>\n",
       "      <td>-0.532096</td>\n",
       "      <td>-0.163504</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>-1.148351</td>\n",
       "      <td>0.95523</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>-1.105008</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.415577</td>\n",
       "      <td>-0.441000</td>\n",
       "      <td>-0.199816</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>0.092415</td>\n",
       "      <td>-0.422702</td>\n",
       "      <td>-0.023154</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>-1.148351</td>\n",
       "      <td>0.95523</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>-1.105008</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.685702</td>\n",
       "      <td>-0.733025</td>\n",
       "      <td>-0.288240</td>\n",
       "      <td>-0.133886</td>\n",
       "      <td>-0.057212</td>\n",
       "      <td>-0.629416</td>\n",
       "      <td>-0.146956</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>-1.148351</td>\n",
       "      <td>0.95523</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>-1.105008</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.668289</td>\n",
       "      <td>-0.677004</td>\n",
       "      <td>-0.386083</td>\n",
       "      <td>-0.212368</td>\n",
       "      <td>-0.110722</td>\n",
       "      <td>-0.584201</td>\n",
       "      <td>-0.199250</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>-1.148351</td>\n",
       "      <td>0.95523</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>-1.105008</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.821525</td>\n",
       "      <td>-0.894904</td>\n",
       "      <td>-0.339823</td>\n",
       "      <td>-0.192117</td>\n",
       "      <td>-0.147810</td>\n",
       "      <td>-0.735597</td>\n",
       "      <td>-0.222946</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>-1.148351</td>\n",
       "      <td>0.95523</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>-1.105008</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reflectance_0  reflectance_1  reflectance_2  reflectance_3  reflectance_4  \\\n",
       "7       -0.681429      -0.668911      -0.263843      -0.168792      -0.060708   \n",
       "10      -0.415577      -0.441000      -0.199816       0.010904       0.092415   \n",
       "18      -0.685702      -0.733025      -0.288240      -0.133886      -0.057212   \n",
       "24      -0.668289      -0.677004      -0.386083      -0.212368      -0.110722   \n",
       "25      -0.821525      -0.894904      -0.339823      -0.192117      -0.147810   \n",
       "\n",
       "    reflectance_5  reflectance_6   solar_0   solar_1  solar_2   solar_3  \\\n",
       "7       -0.532096      -0.163504  0.674068 -1.148351  0.95523  0.205882   \n",
       "10      -0.422702      -0.023154  0.674068 -1.148351  0.95523  0.205882   \n",
       "18      -0.629416      -0.146956  0.674068 -1.148351  0.95523  0.205882   \n",
       "24      -0.584201      -0.199250  0.674068 -1.148351  0.95523  0.205882   \n",
       "25      -0.735597      -0.222946  0.674068 -1.148351  0.95523  0.205882   \n",
       "\n",
       "     solar_4  id         y  id2  \n",
       "7  -1.105008   1 -3.998082    7  \n",
       "10 -1.105008   1 -3.998082   10  \n",
       "18 -1.105008   1 -3.998082   18  \n",
       "24 -1.105008   1 -3.998082   24  \n",
       "25 -1.105008   1 -3.998082   25  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the training set without the outliers\n",
    "\n",
    "RMSE_array = np.asarray(RMSE)\n",
    "outliers = np.argwhere(RMSE_array > 7)\n",
    "outliers = np.reshape(outliers, (np.shape(outliers)[0],))\n",
    "\n",
    "train2 = pd.concat([train,df_id2], axis=1)\n",
    "\n",
    "train2_with_instances_removed = train2[train2['id2'].isin(np.asarray(kept_instances))]\n",
    "train2_with_bags_removed = train2_with_instances_removed[train2_with_instances_removed['id'].isin(outliers)==False]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET THE MEAN AND STD OF THE NEW TRAINING SET\n",
    "\n",
    "#mean_kept_y = []\n",
    "#sd_kept_y = []\n",
    "\n",
    "#for i in range(1,981):\n",
    " #   xx3 = (df_kept_y_hat[df_kept_y_hat[\"id\"] == i])\n",
    "    #mean_kept_y.append((np.mean(xx3.loc[:,\"y_hat\"])))\n",
    "     #sd_kept_y.append((np.std(xx3.loc[:,\"y_hat\"])))\n",
    "    \n",
    "#for i in range(0,980):\n",
    "   # numpy.random.normal(loc = mean_kept_y[i], scale = sd_kept_y[i], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_y_dict = dict(zip(train[\"id\"], train[\"y\"]))\n",
    "\n",
    "train_pred[\"y\"] = train_pred[\"id\"].replace(id_y_dict)\n",
    "\n",
    "np.shape(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_y = []\n",
    "predicted_y_training = []\n",
    "\n",
    "\n",
    "for i in range(1,981):\n",
    "    xx = (train_pred[train_pred[\"id\"] == i])\n",
    "    true_y.append(xx.iloc[0,2])\n",
    "    \n",
    "for i in range(1,981):\n",
    "    xx = (train_pred[train_pred[\"id\"] == i])\n",
    "    predicted_y_training.append(xx.loc[:,\"y_hat\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(predicted_y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURES2 = []\n",
    "COLUMNS2 = []\n",
    "for i in range(1,101):\n",
    "    COLUMNS2.append(str(i))\n",
    "    FEATURES2.append(str(i))\n",
    "    \n",
    "LABEL = [\"y\"]\n",
    "ID = [\"id\"]\n",
    "\n",
    "id_column = []\n",
    "for i in range(1,981):\n",
    "    id_column.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_y_trainingArray = np.asarray(predicted_y_training)\n",
    "true_yArray = np.asarray(true_y)\n",
    "id_columnArray = np.asarray(id_column)\n",
    "\n",
    "df_predicted_y = pd.DataFrame(predicted_y_trainingArray, columns=COLUMNS2)\n",
    "df_true_y =  pd.DataFrame(true_yArray, columns=LABEL)\n",
    "df_id_columnArray = pd.DataFrame(id_columnArray, columns=ID)\n",
    "\n",
    "df_predicted_true = pd.concat([df_predicted_y,df_id_columnArray,df_true_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>-6.021400</td>\n",
       "      <td>-3.840708</td>\n",
       "      <td>-3.536518</td>\n",
       "      <td>-3.335321</td>\n",
       "      <td>-3.714761</td>\n",
       "      <td>-3.848545</td>\n",
       "      <td>-2.687434</td>\n",
       "      <td>-3.681495</td>\n",
       "      <td>-3.477635</td>\n",
       "      <td>-3.503855</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.814636</td>\n",
       "      <td>-3.475733</td>\n",
       "      <td>-4.224454</td>\n",
       "      <td>-3.460755</td>\n",
       "      <td>-4.086445</td>\n",
       "      <td>-3.319243</td>\n",
       "      <td>-3.205924</td>\n",
       "      <td>-4.078343</td>\n",
       "      <td>976</td>\n",
       "      <td>-3.666263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>-3.300596</td>\n",
       "      <td>-3.448747</td>\n",
       "      <td>-3.385780</td>\n",
       "      <td>-3.117596</td>\n",
       "      <td>-3.394598</td>\n",
       "      <td>-3.508353</td>\n",
       "      <td>-3.417411</td>\n",
       "      <td>-3.473523</td>\n",
       "      <td>-2.616889</td>\n",
       "      <td>-3.480860</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.452243</td>\n",
       "      <td>-3.484011</td>\n",
       "      <td>-3.357871</td>\n",
       "      <td>-3.232270</td>\n",
       "      <td>-3.082731</td>\n",
       "      <td>-2.985777</td>\n",
       "      <td>-3.214181</td>\n",
       "      <td>-3.558757</td>\n",
       "      <td>977</td>\n",
       "      <td>-3.906247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-1.835551</td>\n",
       "      <td>-1.716611</td>\n",
       "      <td>-1.585653</td>\n",
       "      <td>-2.067531</td>\n",
       "      <td>-1.955666</td>\n",
       "      <td>-1.631939</td>\n",
       "      <td>-2.023438</td>\n",
       "      <td>-1.682725</td>\n",
       "      <td>-1.991886</td>\n",
       "      <td>-2.184844</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.692677</td>\n",
       "      <td>-2.812050</td>\n",
       "      <td>-2.197767</td>\n",
       "      <td>-1.502378</td>\n",
       "      <td>-1.826462</td>\n",
       "      <td>-2.369208</td>\n",
       "      <td>-2.731704</td>\n",
       "      <td>-2.016426</td>\n",
       "      <td>978</td>\n",
       "      <td>-1.888213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>-3.684743</td>\n",
       "      <td>-4.154680</td>\n",
       "      <td>-4.023054</td>\n",
       "      <td>-3.909610</td>\n",
       "      <td>-4.148894</td>\n",
       "      <td>-3.692530</td>\n",
       "      <td>-4.099753</td>\n",
       "      <td>-3.815801</td>\n",
       "      <td>-3.919218</td>\n",
       "      <td>-4.093242</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.917410</td>\n",
       "      <td>-4.316907</td>\n",
       "      <td>-3.973176</td>\n",
       "      <td>-3.732786</td>\n",
       "      <td>-3.885842</td>\n",
       "      <td>-4.290096</td>\n",
       "      <td>-3.822150</td>\n",
       "      <td>-3.908524</td>\n",
       "      <td>979</td>\n",
       "      <td>-4.532433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-3.652737</td>\n",
       "      <td>-4.109573</td>\n",
       "      <td>-4.194172</td>\n",
       "      <td>-4.014184</td>\n",
       "      <td>-3.695538</td>\n",
       "      <td>-4.110124</td>\n",
       "      <td>-4.124703</td>\n",
       "      <td>-4.052232</td>\n",
       "      <td>-3.461169</td>\n",
       "      <td>-3.597208</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.335089</td>\n",
       "      <td>-4.173898</td>\n",
       "      <td>-4.150982</td>\n",
       "      <td>-4.144796</td>\n",
       "      <td>-3.406513</td>\n",
       "      <td>-4.125513</td>\n",
       "      <td>-4.152604</td>\n",
       "      <td>-3.368629</td>\n",
       "      <td>980</td>\n",
       "      <td>-4.744636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2         3         4         5         6         7  \\\n",
       "975 -6.021400 -3.840708 -3.536518 -3.335321 -3.714761 -3.848545 -2.687434   \n",
       "976 -3.300596 -3.448747 -3.385780 -3.117596 -3.394598 -3.508353 -3.417411   \n",
       "977 -1.835551 -1.716611 -1.585653 -2.067531 -1.955666 -1.631939 -2.023438   \n",
       "978 -3.684743 -4.154680 -4.023054 -3.909610 -4.148894 -3.692530 -4.099753   \n",
       "979 -3.652737 -4.109573 -4.194172 -4.014184 -3.695538 -4.110124 -4.124703   \n",
       "\n",
       "            8         9        10    ...           93        94        95  \\\n",
       "975 -3.681495 -3.477635 -3.503855    ...    -3.814636 -3.475733 -4.224454   \n",
       "976 -3.473523 -2.616889 -3.480860    ...    -3.452243 -3.484011 -3.357871   \n",
       "977 -1.682725 -1.991886 -2.184844    ...    -1.692677 -2.812050 -2.197767   \n",
       "978 -3.815801 -3.919218 -4.093242    ...    -3.917410 -4.316907 -3.973176   \n",
       "979 -4.052232 -3.461169 -3.597208    ...    -3.335089 -4.173898 -4.150982   \n",
       "\n",
       "           96        97        98        99       100   id         y  \n",
       "975 -3.460755 -4.086445 -3.319243 -3.205924 -4.078343  976 -3.666263  \n",
       "976 -3.232270 -3.082731 -2.985777 -3.214181 -3.558757  977 -3.906247  \n",
       "977 -1.502378 -1.826462 -2.369208 -2.731704 -2.016426  978 -1.888213  \n",
       "978 -3.732786 -3.885842 -4.290096 -3.822150 -3.908524  979 -4.532433  \n",
       "979 -4.144796 -3.406513 -4.125513 -4.152604 -3.368629  980 -4.744636  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_excl2 = [\"y\",\"id\"]\n",
    "cols_orig2 = [c for c in df_predicted_true.columns if c not in cols_excl2]\n",
    "\n",
    "df_predicted_true.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat_test = list(model_dnn.predict(input_fn=lambda: input_fn(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_y_dict2 = dict(zip(test[\"id\"], test[\"y\"]))\n",
    "\n",
    "train_pred2[\"y\"] = train_pred2[\"id\"].replace(id_y_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_y = []\n",
    "predicted_y_training = []\n",
    "\n",
    "\n",
    "for i in range(1,981):\n",
    "    xx = (train_pred[train_pred[\"id\"] == i])\n",
    "    true_y.append(xx.iloc[0,2])\n",
    "    \n",
    "for i in range(1,981):\n",
    "    xx = (train_pred[train_pred[\"id\"] == i])\n",
    "    predicted_y_training.append(xx.loc[:,\"y_hat\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y_trainingArray = np.asarray(predicted_y_training)\n",
    "true_yArray = np.asarray(true_y)\n",
    "id_columnArray = np.asarray(id_column)\n",
    "\n",
    "df_predicted_y = pd.DataFrame(predicted_y_trainingArray, columns=COLUMNS2)\n",
    "df_true_y =  pd.DataFrame(true_yArray, columns=LABEL)\n",
    "df_id_columnArray = pd.DataFrame(id_columnArray, columns=ID)\n",
    "\n",
    "df_predicted_true = pd.concat([df_predicted_y,df_id_columnArray,df_true_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_excl2 = [\"y\",\"id\"]\n",
    "cols_orig2 = [c for c in df_predicted_true.columns if c not in cols_excl2]\n",
    "\n",
    "df_predicted_true.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
